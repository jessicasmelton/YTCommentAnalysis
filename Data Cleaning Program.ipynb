{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcvHD0Cz/r48QppKIqsI1a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessicasmelton/YTCommentAnalysis/blob/main/Data%20Cleaning%20Program.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning Program:**\n",
        "\n",
        "This program is designed to clean and preprocess YouTube comment data extracted from an Excel file. The program performs several key operations: it converts text to lowercase, removes punctuation and special characters, and eliminates common English stop words. The cleaned comments are then saved to a new CSV file for further analysis.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Usage**\n",
        "\n",
        "* Ensure your Excel file is correctly formatted and saved in the specified location. The program skips the first row due to a known issue with reading columns.\n",
        "\n",
        "* Replace the input_file variable value in the code with the path to your Excel file. Make sure the file path is correctly specified to avoid file not found errors.\n",
        "\n",
        "* Execute the program in a Python environment such as Google Colab, Jupyter Notebook, or any local Python environment.\n",
        "\n",
        "* The program reads the Excel file, processes the comments, and saves the cleaned data to a new CSV file.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Notes**\n",
        "* The program checks for the existence of a column named Comment Text in the combined DataFrame. If the column is missing, the program raises a KeyError.\n",
        "\n",
        "* The program uses a basic set of English stop words. You can customize the stop_words set in the preprocess_text function to include additional stop words as needed.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Potential Errors and Fixes**\n",
        "\n",
        "* Ensure the file path to the Excel file is correct. Verify that the file exists at the specified location.\n",
        "\n",
        "* If the 'Comment Text' column is missing, ensure that all sheets in the Excel file contain this column. The program relies on this column for preprocessing.\n",
        "\n",
        "* If there are issues with saving the CSV file, check for special characters in the file path or name that may cause problems. Ensure the directory where the file is being saved exists and is writable."
      ],
      "metadata": {
        "id": "CDqsP1IcddTe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKoEiPQKdcnE"
      },
      "outputs": [],
      "source": [
        "# Data Cleaning Program (for Excel Files)\n",
        "\n",
        "# This program skips the first row because there was an error\n",
        "# reading the columns in the the first interation of this program.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd  # Library for data manipulation and analysis\n",
        "import string  # Library for string operations\n",
        "\n",
        "# Function to preprocess the text in the 'Comment Text' column\n",
        "def preprocess_text(text):\n",
        "    # Check if the input is a string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"  # Return empty string if the input is not a string\n",
        "\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and special characters using string translation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text by splitting it into words (tokens) based on spaces\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Define a basic set of stop words to be removed from the text\n",
        "    stop_words = set([\n",
        "        'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
        "        'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
        "        'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them',\n",
        "        'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',\n",
        "        'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
        "        'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
        "        'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
        "        'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
        "        'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to',\n",
        "        'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
        "        'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
        "        'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n",
        "        'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',\n",
        "        'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
        "    ])\n",
        "    # Remove stop words from the tokenized text\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Join the cleaned tokens back into a single string\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "    return cleaned_text\n",
        "\n",
        "# Read the Excel file, skipping the first row due to an error in reading columns initially\n",
        "input_file = 'INSERT FILE PATH HERE.xlsx'\n",
        "df = pd.read_excel(input_file, sheet_name=None, skiprows=1)  # Skip the first row\n",
        "\n",
        "# Initialize an empty DataFrame to combine data from all sheets\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# Iterate over each sheet in the Excel file and combine them into one DataFrame\n",
        "for sheet_name, sheet_df in df.items():\n",
        "    combined_df = pd.concat([combined_df, sheet_df], ignore_index=True)\n",
        "\n",
        "# Strip any leading/trailing spaces from column names to ensure consistency\n",
        "combined_df.columns = combined_df.columns.str.strip()\n",
        "\n",
        "# Verify that the 'Comment Text' column exists in the combined DataFrame\n",
        "if 'Comment Text' not in combined_df.columns:\n",
        "    raise KeyError(\"The combined data does not contain a column named 'Comment Text'. Please check the column names.\")\n",
        "\n",
        "# Apply the preprocessing function to the 'Comment Text' column and create a new column 'Cleaned Comment Text'\n",
        "combined_df['Cleaned Comment Text'] = combined_df['Comment Text'].apply(preprocess_text)\n",
        "\n",
        "# Save the cleaned DataFrame to a new CSV file\n",
        "output_file = '/CLEANED_Youtube_Comment_Data.csv'\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "\n",
        "# Print a completion message indicating the location of the saved file\n",
        "print(f\"Preprocessed data has been saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning Program That Translate Non-English Comments to English:**\n",
        "\n",
        "This program translates YouTube comments from various languages to English and then cleans the translated comments. It handles text processing tasks such as converting text to lowercase, removing punctuation, eliminating stop words, and applying lemmatization. The processed comments are saved to a new CSV file.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Usage**\n",
        "\n",
        "* Ensure your CSV file (from the previous cleaning step) is correctly formatted and saved in the specified location. The file should contain the cleaned comments from the first data cleaning program.\n",
        "\n",
        "* Replace the file_path variable value in the code with the path to your CSV file. Make sure the file path is correctly specified to avoid file not found errors.\n",
        "\n",
        "* Execute the program in a Python environment such as Google Colab, Jupyter Notebook, or any local Python environment.\n",
        "\n",
        "* The program reads the CSV file, processes the comments, and saves the cleaned and translated data to a new CSV file.\n",
        "\n",
        "**Notes**\n",
        "\n",
        "* The program assumes the CSV file contains a column named Comment Text. If the column is missing, the program will not function correctly.\n",
        "Language Detection:\n",
        "\n",
        "* The program detects the language of each comment and translates non-English comments to English. Very short comments or those that cannot be detected are marked as 'unknown'.\n",
        "\n",
        "* The program uses a basic set of English stop words. You can customize the stop_words set in the remove_stopwords function to include additional stop words as needed.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Potential Errors and Fixes**\n",
        "\n",
        "* Ensure the file path to the CSV file is correct. Verify that the file exists at the specified location.\n",
        "\n",
        "* If the 'Comment Text' column is missing, ensure that the CSV file contains this column. The program relies on this column for preprocessing and translation.\n",
        "\n",
        "* If translation errors occur, they are logged, and the original text is retained. Check the console output for specific error messages.\n",
        "\n",
        "* If there are issues with saving the CSV file, check for special characters in the file path or name that may cause problems. Ensure the directory where the file is being saved exists and is writable."
      ],
      "metadata": {
        "id": "xTcJqzwXlwTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install First\n",
        "\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install nltk\n",
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "aCkTcRQnl6HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # Library for data manipulation and analysis\n",
        "from googletrans import Translator  # Library for translating text\n",
        "import nltk  # Natural Language Toolkit for text processing\n",
        "from nltk.corpus import stopwords  # Module for stop words\n",
        "from nltk.tokenize import word_tokenize  # Module for tokenizing text\n",
        "from nltk.stem import WordNetLemmatizer  # Module for lemmatizing words\n",
        "from langdetect import detect, LangDetectException  # Library for detecting language\n",
        "import re  # Regular expressions library for text cleaning\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the cleaned CSV file containing YouTube comments\n",
        "file_path = '/content/Cleaned_Youtube_Comments.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize the Google Translator and NLTK Lemmatizer\n",
        "translator = Translator()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to clean text by converting to lowercase, removing punctuation, special characters, and numbers\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation and special characters\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    return text\n",
        "\n",
        "# Function to remove stop words from text\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))  # Set of English stop words\n",
        "    word_tokens = word_tokenize(text)  # Tokenize text into words\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]  # Remove stop words\n",
        "    return filtered_text\n",
        "\n",
        "# Function to apply lemmatization to a list of words\n",
        "def apply_lemmatization(words):\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize words\n",
        "    return ' '.join(lemmatized_words)  # Join lemmatized words back into a single string\n",
        "\n",
        "# Function to translate text to English, clean it, remove stop words, and apply lemmatization\n",
        "def translate_and_clean(text):\n",
        "    try:\n",
        "        translated = translator.translate(text, dest='en').text  # Translate text to English\n",
        "        cleaned_text = clean_text(translated)  # Clean the translated text\n",
        "        tokens = remove_stopwords(cleaned_text)  # Remove stop words from cleaned text\n",
        "        lemmatized_text = apply_lemmatization(tokens)  # Apply lemmatization to the tokens\n",
        "        return lemmatized_text  # Return the processed text\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {text}\\nError: {e}\")  # Print error message if processing fails\n",
        "        return text  # Return the original text if processing fails\n",
        "\n",
        "# Function to detect the language of text with checks for empty or very short comments\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        if len(text.strip()) < 3:  # Skip very short texts\n",
        "            return 'unknown'\n",
        "        return detect(text)  # Detect the language of the text\n",
        "    except LangDetectException:\n",
        "        return 'unknown'  # Return 'unknown' if language detection fails\n",
        "\n",
        "# Apply language detection to each comment and create a new column 'Language'\n",
        "df['Language'] = df['Comment Text'].apply(detect_language)\n",
        "\n",
        "# Filter out comments that are not in English\n",
        "non_english_df = df[df['Language'] != 'en']\n",
        "\n",
        "# Translate and clean non-English comments\n",
        "non_english_df['Comment Text'] = non_english_df['Comment Text'].apply(translate_and_clean)\n",
        "\n",
        "# Update the original DataFrame with the translated comments\n",
        "df.update(non_english_df)\n",
        "\n",
        "# Drop the 'Language' column from the DataFrame as it is no longer needed\n",
        "df.drop(columns=['Language'], inplace=True)\n",
        "\n",
        "# Save the cleaned and translated comments to a new CSV file\n",
        "output_file_path = '/content/CLEANED_Translated_Youtube_Comments.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Print completion message indicating the location of the saved file\n",
        "print(f\"Cleaned and translated comments have been saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "KDKCCs0tmFq4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
